{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d8adec-d4ad-40d7-857f-2f38e3a53291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "# Sum oparation for reduce by key\n",
    "def sumOparator(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "# Remove replications after cartesian oparation\n",
    "def removeReplica(record):\n",
    "    if isinstance(record[0], tuple):\n",
    "        x1 = record[0]\n",
    "        x2 = record[1]\n",
    "    else:\n",
    "        x1 = [record[0]]\n",
    "        x2 = record[1]\n",
    "\n",
    "    if not any(x == x2 for x in x1):\n",
    "        a = list(x1)\n",
    "        a.append(x2)\n",
    "        a.sort()\n",
    "        result = tuple(a)\n",
    "        return result\n",
    "    else:\n",
    "        return x1\n",
    "\n",
    "\n",
    "# Filter items\n",
    "def filterForConf(item):\n",
    "    if len(item[0][0]) > len(item[1][0]):\n",
    "        if not checkItemSets(item[0][0], item[1][0]):\n",
    "            pass\n",
    "        else:\n",
    "            return item\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Check Items sets includes at least one comman item // Example command: # any(l == k for k in z for l in x )\n",
    "def checkItemSets(item_1, item_2):\n",
    "    if len(item_1) > len(item_2):\n",
    "        return all(any(k == l for k in item_1) for l in item_2)\n",
    "    else:\n",
    "        return all(any(k == l for k in item_2) for l in item_1)\n",
    "\n",
    "\n",
    "# Confidence calculation\n",
    "def calculateConfidence(item):\n",
    "    # Parent item list\n",
    "    parent = set(item[0][0])\n",
    "\n",
    "    # Child item list\n",
    "    if isinstance(item[1][0], str):\n",
    "        child = set([item[1][0]])\n",
    "    else:\n",
    "        child = set(item[1][0])\n",
    "    # Parent and Child support values\n",
    "    parentSupport = item[0][1]\n",
    "    childSupport = item[1][1]\n",
    "    # Finds the item set confidence is going to be found\n",
    "\n",
    "    confidence = (parentSupport / childSupport)\n",
    "\n",
    "    return list([list(child), list(parent.difference(child)), confidence])\n",
    "\n",
    "\n",
    "class Apriori:\n",
    "\n",
    "    def __init__(self, path, sc, minSupport, minConfidence):\n",
    "        if not (0 <= minSupport <= 1):\n",
    "            raise Exception(\"The minimum support should be between 0 and 1\")\n",
    "\n",
    "        if not (0 <= minConfidence <= 1):\n",
    "            raise Exception(\"The minimum confidence should be between 0 and 1\")\n",
    "\n",
    "        # File path\n",
    "        self.confidences = None\n",
    "        self.path = path\n",
    "\n",
    "        # Spark Context\n",
    "        self.sc = sc\n",
    "\n",
    "        self.raw = self.sc.textFile(self.path)\n",
    "        \n",
    "        self.n_samples = self.raw.count()\n",
    "        self.minSupport = self.n_samples * minSupport\n",
    "        self.minConfidence = minConfidence\n",
    "\n",
    "        ## Whole Date set with frequencies\n",
    "        self.lblitems = self.raw.map(lambda line: line.split(','))\n",
    "\n",
    "        ## Whole lines in single array\n",
    "        self.wlitems = self.raw.flatMap(lambda line: line.split(','))\n",
    "        self.wlitems.persist()\n",
    "\n",
    "        ## Unique frequent items in dataset\n",
    "        self.uniqueItems = self.wlitems.distinct()\n",
    "        self.uniqueItems.persist()\n",
    "\n",
    "    def fit(self):\n",
    "        supportRdd = self.wlitems.map(lambda item: (item, 1))\n",
    "        supportRdd = supportRdd.reduceByKey(sumOparator)\n",
    "        supports = supportRdd.map(lambda item: item[1])\n",
    "\n",
    "        # Define minimum support value\n",
    "        if self.minSupport == 'auto':\n",
    "            minSupport = supports.min()\n",
    "        else:\n",
    "            minSupport = self.minSupport\n",
    "\n",
    "        # If minimum support is 1 then replace it with 2\n",
    "        minSupport = 2 if minSupport < 2 else minSupport\n",
    "        \n",
    "        minConfidence = self.minConfidence\n",
    "\n",
    "        # Filter first supportRdd with minimum support\n",
    "        supportRdd = supportRdd.filter(lambda item: item[1] >= minSupport)\n",
    "\n",
    "        # Create base RDD with will be updated every iteration\n",
    "        baseRdd = supportRdd.map(lambda item: ([item[0]], item[1]))\n",
    "        baseRdd.persist()\n",
    "\n",
    "        supportRdd = supportRdd.map(lambda item: item[0])\n",
    "        supportRdd.persist()\n",
    "\n",
    "        c = 2\n",
    "\n",
    "        print(\"Started fitting\")\n",
    "        while not supportRdd.isEmpty():\n",
    "            combined = supportRdd.cartesian(self.uniqueItems).coalesce(100)\n",
    "            combined = combined.map(lambda item: removeReplica(item))\n",
    "\n",
    "            combined = combined.filter(lambda item: len(item) == c)\n",
    "            combined = combined.distinct()\n",
    "            combined.persist()\n",
    "\n",
    "            combined_2 = combined.cartesian(self.lblitems).coalesce(100)\n",
    "            combined_2 = combined_2.filter(lambda item: all(x in item[1] for x in item[0]))\n",
    "\n",
    "            combined_2 = combined_2.map(lambda item: item[0])\n",
    "            combined_2 = combined_2.map(lambda item: (item, 1))\n",
    "            combined_2 = combined_2.reduceByKey(sumOparator)\n",
    "            combined_2 = combined_2.filter(lambda item: item[1] >= minSupport)\n",
    "\n",
    "            baseRdd = baseRdd.union(combined_2)\n",
    "\n",
    "            combined_2 = combined_2.map(lambda item: item[0])\n",
    "            supportRdd = combined_2\n",
    "            supportRdd.persist()\n",
    "            c = c + 1\n",
    "\n",
    "        print(\"Fitting final stage\")\n",
    "        sets = baseRdd.cartesian(baseRdd).coalesce(100)\n",
    "        filtered = sets.filter(lambda item: filterForConf(item))\n",
    "        confidences = filtered.map(lambda item: calculateConfidence(item))\n",
    "        self.confidences = confidences\n",
    "        \n",
    "        rules = confidences.filter(lambda item: item[2] >= minConfidence)\n",
    "        \n",
    "        output_path = f\"/home/ubuntu/data/apriori_output\"\n",
    "        !hadoop fs -rm -R {output_path}\n",
    "        rules.coalesce(1).saveAsTextFile(output_path)\n",
    "\n",
    "        return rules\n",
    "\n",
    "    def predict(self, set, confidence):\n",
    "\n",
    "        if not isinstance(set, list):\n",
    "            raise ValueError('For prediction \"set\" argument should be a list')\n",
    "\n",
    "        _confidences = self.confidences\n",
    "        _filterForPredict = self._filterForPredict\n",
    "\n",
    "        filtered = _confidences.filter(lambda item: _filterForPredict(item, set, confidence))\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    @staticmethod\n",
    "    def _filterForPredict(item, set, confidence):\n",
    "        it = item[0]\n",
    "        it.sort()\n",
    "        set.sort()\n",
    "        if it == set and item[2] >= confidence:\n",
    "            return item\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "def run_with_percentage_of_data(path, percentage, minSupport, minConfidence):\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"Python Spark Apriori\")\n",
    "        .config(\"spark.executor.memory\", \"7680M\")\n",
    "        .config(\"spark.driver.memory\", \"9G\")\n",
    "        .config(\"spark.executor.cores\", \"2\")\n",
    "        #mai multe la https://spark.apache.org/docs/latest/configuration.html\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    # Percentage split\n",
    "    lines = open(path, \"r\").readlines()\n",
    "    lines = lines[:int(len(lines) * percentage)]\n",
    "    \n",
    "    #Write to temp file\n",
    "    filename = 'apriori_split'\n",
    "    local_path = f\"./temp/{filename}.txt\"\n",
    "    hadoop_path = f\"/home/ubuntu/data/{filename}.txt\"\n",
    "    \n",
    "    with open(local_path, \"w\") as f:\n",
    "        f.writelines(lines)\n",
    "        \n",
    "    # Upload to HDFS otherwise it won't work\n",
    "    !hadoop fs -put -f {local_path} {hadoop_path}\n",
    "\n",
    "    # Construct Apriori\n",
    "    apriori = Apriori(hadoop_path, sc, minSupport=minSupport, minConfidence=minConfidence)\n",
    "    \n",
    "    print(\"Loaded data. Starting fitting!\")\n",
    "    supports = apriori.fit()\n",
    "    \n",
    "    for i in supports.take(10):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc5805a-ca4b-452a-894c-6eb0a1b3e68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data. Starting fitting!\n",
      "Started fitting\n",
      "Fitting final stage\n",
      "Deleted /home/ubuntu/data/apriori_output\n",
      "[['17'], ['14'], 0.10517322649069055]\n",
      "[['17'], ['1'], 0.443082724487391]\n",
      "[['17'], ['1', '3'], 0.11583785057742163]\n",
      "[['7', '2'], ['14'], 0.1834521538307059]\n",
      "[['5', '3'], ['1'], 0.5723925197609409]\n",
      "[['11', '6'], ['1'], 0.5697290725946509]\n",
      "[['11', '15'], ['1'], 0.5238556551223057]\n",
      "[['14', '3'], ['10', '12'], 0.12226163154600458]\n",
      "[['4', '8', '2'], ['1'], 0.4227793252862891]\n",
      "[['10', '3', '6'], ['5'], 0.3551155115511551]\n"
     ]
    }
   ],
   "source": [
    "run_with_percentage_of_data(\"./data/msnbc.csv\", 1, 0.001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1353370-3e92-4edf-a403-7de70d28cb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
